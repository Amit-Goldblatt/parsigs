{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 16:13:14.026789: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.cli.train import train\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "_ = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "_ = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: sig_ner_bert2_model\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: sig_ner_bert2_model\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0       29214.57   1228.53    0.09    0.11    0.08    0.00\n",
      "  3     200      595007.49  86022.72   63.14   76.26   53.87    0.63\n",
      "  6     400       53470.24  34592.91   86.61   84.83   88.48    0.87\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "sig_ner_bert2_model/model-last\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023-04-20 09:51:19.067891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[38;5;4mℹ Saving to output directory: sig_bert_model\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-04-20 09:51:27,813] [INFO] Set up nlp object from config\n",
      "[2023-04-20 09:51:27,829] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2023-04-20 09:51:27,832] [INFO] Created vocabulary\n",
      "[2023-04-20 09:51:27,833] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2023-04-20 09:51:35,864] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0       29214.57   1228.53    0.09    0.11    0.08    0.00\n",
      "  3     200      595007.49  86022.72   63.14   76.26   53.87    0.63\n",
      "  6     400       53470.24  34592.91   86.61   84.83   88.48    0.87\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "sig_bert_model/model-last\n"
     ]
    }
   ],
   "source": [
    "train(\"config/config_bert.cfg\",\n",
    "      output_path='sig_ner_bert2_model',\n",
    "      overrides={\"paths.train\": \"train_docs.spacy\", \n",
    "                 \"paths.dev\": \"test_docs.spacy\",\n",
    "                 \"training.max_epochs\": 10})\n",
    "\n",
    "!python3 -m spacy train config/config_bert.cfg -o sig_bert_model --paths.train train_docs.spacy --paths.dev test_docs.spacy --training.max_epochs 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_nlp = spacy.load('./sig_bert_model/model-best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first outcome! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Dosage'),\n",
       " (tablet, 'Form'),\n",
       " (150 mcg, 'Strength'),\n",
       " (two times daily, 'Frequency'),\n",
       " (for one week, 'Duration'),\n",
       " (2, 'Dosage'),\n",
       " (aderol, 'Drug'),\n",
       " (every day, 'Frequency')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 'TAKE 1 TABLET  (150 MCG TOTAL) BY MOUTH two times DAILY for one week by the way patient id is 303023 and take 2 aderol every day'.lower()\n",
    "[(e, e.label_) for e in trained_nlp(inp).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'Dosage'), (tablets, 'Form'), (aderol, 'Drug'), (every week, 'Frequency')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 'Take 2 tablets of aderol in the morning every week'.lower()\n",
    "[(e, e.label_) for e in trained_nlp(inp).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'Dosage'),\n",
       " (capsules, 'Form'),\n",
       " (qid, 'Frequency'),\n",
       " (amoxicillin, 'Drug'),\n",
       " (500mg, 'Strength')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Take 2 capsules qid of amoxicillin 500mg\"\n",
    "[(e, e.label_) for e in trained_nlp(inp).ents]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can even parse multuple dosing instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3 puffs, 'Dosage'), (albuterol, 'Drug'), (2 times a day, 'Frequency'), (for one week, 'Duration'), (1 puff, 'Dosage'), (albuterol, 'Drug'), (every day, 'Frequency'), (for 2 months, 'Duration')]\n"
     ]
    }
   ],
   "source": [
    "inp = 'inhale 3 puffs of albuterol 2 times a day for one week' \\\n",
    "      ' then 1 puff of albuterol every day for 2 months'\n",
    "\n",
    "print([(e, e.label_) for e in trained_nlp(inp).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tablet, 'Form'), (20 mg, 'Strength')]\n"
     ]
    }
   ],
   "source": [
    "inp = 'TAKE 1 TABLET (20 MG) BY MOUTH every 2 months'.lower()\n",
    "print([(e, e.label_) for e in trained_nlp(inp).ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are many things to improve, one example is the frequency that can and should be parsed to an interval type and amoubt (e.g type=day, amount=2)\n",
    "\n",
    "Also, the model only recognizes encountered features, so inputting to the model a new sentence, will not be parsed correctly. \n",
    "When we will use the pre-trained Bert model, this issue should be resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Dosage'),\n",
       " (tablet, 'Form'),\n",
       " (ibuprofen, 'Drug'),\n",
       " (3 times a day, 'Dosage')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 'Take 1 tablet of ibuprofen 3 times a day'\n",
    "\n",
    "\n",
    "[(e, e.label_) for e in trained_nlp(inp).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Dosage'), (q7d, 'Frequency')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = '1 TAB Q7d'.lower()\n",
    "\n",
    "[(e, e.label_) for e in trained_nlp(inp).ents]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
